services:
  n8n:
    build: .
    ports:
      - "5678:5678"
    volumes:
      - ./n8n_data:/home/node/.n8n
      - ./documentos:/data/documentos
      - ./documents:/data/documents
      - ./tmp:/data/
    env_file:
      - .env
    environment:
      N8N_SECURE_COOKIE: "false"
      N8N_BASIC_AUTH_ACTIVE: "true"
      N8N_BASIC_AUTH_USER: "jesus"
      N8N_BASIC_AUTH_PASSWORD: "airtrace"
      # Permitir libs en Code node (sin qdrant, sin fetch)
      NODE_FUNCTION_ALLOW_EXTERNAL: "axios,csv2xlsx,pg,pg-format"
      TZ: "Europe/Madrid"
      # Conexi√≥n a Postgres para el Code node
      PGHOST: "postgres"
      PGPORT: "5432"
      PGDATABASE: "postgres"
      PGUSER: "raguser"
      PGPASSWORD: "ragpass123"
      # URL de Ollama para el Code node
      OLLAMA_URL: "http://ollama:11434"
      OPENROUTER_API_KEY: "${OPENROUTER_API_KEY}"
      OPENROUTER_HTTP_REFERER: "https://airtrace.io"
      OPENROUTER_X_TITLE: "AirTrace n8n Ingestion"
    depends_on:
      - ollama
      - postgres

  ollama:
    image: ollama/ollama:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
    container_name: ollama
    restart: unless-stopped

  postgres:
    image: pgvector/pgvector:pg16
    container_name: rag_postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: postgres
      POSTGRES_USER: raguser
      POSTGRES_PASSWORD: ragpass123
    ports:
      - "5432:5432"
    volumes:
      - ./pgvector_data:/var/lib/postgresql/data
      - ./sql/schema.sql:/docker-entrypoint-initdb.d/01-schema.sql
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U raguser -d postgres" ]
      interval: 5s
      timeout: 5s
      retries: 20

  adminer:
    image: adminer
    restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      - ADMINER_DEFAULT_SERVER=postgres
    depends_on:
      - postgres

  rag-agent:
    build:
      context: .
      dockerfile: Dockerfile-rag
    container_name: rag_voice_agent
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_started
    environment:
      DATABASE_URL: postgresql://raguser:ragpass123@postgres:5432/postgres
      OLLAMA_URL: http://ollama:11434
      LLM_PROVIDER: ollama
      CHAT_MODEL: llama3.1
      EMBED_MODEL: nomic-embed-text
      OPENAI_API_KEY: ""#${OPENAI_API_KEY}
      LIVEKIT_URL: ${LIVEKIT_URL}
      LIVEKIT_API_KEY: ${LIVEKIT_API_KEY}
      LIVEKIT_API_SECRET: ${LIVEKIT_API_SECRET}
      DEEPGRAM_API_KEY: ${DEEPGRAM_API_KEY}
      USE_HYBRID_CHUNKER: "0"
      DOCLING_DISABLE_OCR: "0"
      DOCLING_PICTURE_DESCRIPTIONS: "vlm"
      DOCLING_PICTURE_DESCRIPTION_OPTIONS__SCALE: "page"
      DOCLING_PICTURE_DESCRIPTION_OPTIONS__PROMPT: "brief-captions"
      DOCLING_PICTURE_DESCRIPTION_API_OPTIONS__TYPE: "api"
      DOCLING_PDF_PIPELINE_OPTIONS__ENABLE_REMOTE_SERVICES: "true"
      DOCLING_PICTURE_AREA_THRESHOLD: "0.1"
      DOCLING_VLM_API_URL: "http://ollama:11434/v1/chat/completions"
      DOCLING_VLM_API_PROVIDER: ollama
      DOCLING_VLM_API_MODEL: "llava:7b"
      DOCLING_OCR_OPTIONS__LANG: "['fr','de','es','en']"
      LOG_LEVEL: INFO
    tty: true
    stdin_open: true
    volumes:
      - ./documents:/app/documents
      - ./.env:/app/.env
    command: [ "python", "cli.py" ]
    restart: unless-stopped

  ingestion:
    build:
      context: .
      dockerfile: Dockerfile-rag
    container_name: rag_ingestion
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_started
    environment:
      DATABASE_URL: postgresql://raguser:ragpass123@postgres:5432/postgres
      OLLAMA_URL: http://ollama:11434
      LLM_PROVIDER: ollama
      EMBED_MODEL: nomic-embed-text
      CHAT_MODEL: llama3.1
      USE_HYBRID_CHUNKER: "0"
      DOCLING_DISABLE_OCR: "0"
      DOCLING_PICTURE_DESCRIPTIONS: "vlm"
      DOCLING_PICTURE_DESCRIPTION_OPTIONS__SCALE: "page"
      DOCLING_PICTURE_DESCRIPTION_OPTIONS__PROMPT: "brief-captions"
      DOCLING_PICTURE_DESCRIPTION_API_OPTIONS__TYPE: "api"
      DOCLING_PDF_PIPELINE_OPTIONS__ENABLE_REMOTE_SERVICES: "true"
      DOCLING_PICTURE_AREA_THRESHOLD: "0.1"
      DOCLING_VLM_API_PROVIDER: ollama
      DOCLING_VLM_API_MODEL: "llava:7b"
      DOCLING_OCR_OPTIONS__LANG: "['fr','de','es','en']"
    volumes:
      - ./documents:/app/documents
      - ./.env:/app/.env
    command: [ "uvicorn", "ingestion.api:app", "--host", "0.0.0.0", "--port", "8001" ]
    restart: unless-stopped

  docling:
    image: ghcr.io/docling-project/docling-serve-cpu:latest
    container_name: docling
    ports:
      - "5001:5001"
    environment:
      # B√°sicos
      DOCLING_SERVE_ENABLE_UI: "1"
      DOCLING_OCR_LANGS: "es,en"
      DOCLING_SERVE_MAX_SYNC_WAIT: "600"

      # üëâ Activa servicios remotos y captions VLM **en el pipeline est√°ndar**
      DOCLING_STANDARD_PDF_PIPELINE_OPTIONS__ENABLE_REMOTE_SERVICES: "true"
      DOCLING_STANDARD_PDF_PIPELINE_OPTIONS__DO_PICTURE_DESCRIPTION: "true"
      DOCLING_STANDARD_PDF_PIPELINE_OPTIONS__PICTURE_DESCRIPTIONS: "vlm"
      DOCLING_STANDARD_PDF_PIPELINE_OPTIONS__PICTURE_AREA_THRESHOLD: "0.03"  # sube/baja sensibilidad

      # üëâ Opciones de ‚Äúdescriber‚Äù (estilo / escala)
      DOCLING_STANDARD_PDF_PIPELINE_OPTIONS__PICTURE_DESCRIPTION_OPTIONS__SCALE: "page"
      DOCLING_STANDARD_PDF_PIPELINE_OPTIONS__PICTURE_DESCRIPTION_OPTIONS__PROMPT: "brief-captions"

      # üëâ VLM remoto v√≠a OpenRouter
      DOCLING_STANDARD_PDF_PIPELINE_OPTIONS__PICTURE_DESCRIPTION_API_OPTIONS__TYPE: "api"
      DOCLING_STANDARD_PDF_PIPELINE_OPTIONS__PICTURE_DESCRIPTION_API_OPTIONS__URL: "https://openrouter.ai/api/v1/chat/completions"
      DOCLING_STANDARD_PDF_PIPELINE_OPTIONS__PICTURE_DESCRIPTION_API_OPTIONS__HEADERS__AUTHORIZATION: "Bearer ${OPENROUTER_API_KEY}"
      DOCLING_STANDARD_PDF_PIPELINE_OPTIONS__PICTURE_DESCRIPTION_API_OPTIONS__HEADERS__HTTP-REFERER: "https://airtrace.io"
      DOCLING_STANDARD_PDF_PIPELINE_OPTIONS__PICTURE_DESCRIPTION_API_OPTIONS__HEADERS__X-TITLE: "AirTrace n8n Ingestion"
      DOCLING_STANDARD_PDF_PIPELINE_OPTIONS__PICTURE_DESCRIPTION_API_OPTIONS__PARAMS__MODEL: "google/gemma-3-27b-it:free"
      DOCLING_STANDARD_PDF_PIPELINE_OPTIONS__PICTURE_DESCRIPTION_API_OPTIONS__PARAMS__MAX_TOKENS: "64"

      # üëâ Ruta fija para artefactos (cach√© de modelos OCR, etc.)
      DOCLING_SERVE_ARTIFACTS_PATH: "/opt/app-root/src/.cache/docling/models"
    volumes:
      - docling_cache:/opt/app-root/src/.cache
      - ./documents:/opt/app-root/documents
    restart: unless-stopped



  weave-worker:
    build:
      context: ./weave-worker
      dockerfile: Dockerfile
    image: weave-worker:v1
    container_name: weave-worker
    environment:
      - WEAVE_WANDB_KEY=${WEAVE_WANDB_KEY}
      - WEAVE_PROJECT=${WEAVE_PROJECT}
    ports:
      - "7000:7000"
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant
    hostname: qdrant
    container_name: qdrant
    restart: unless-stopped
    ports:
      - 6333:6333
    volumes:
      - ./qdrant_storage:/qdrant/storage

volumes:
  n8n_data:
  ollama_data:
  pgvector_data:
  qdrant_storage:
  docling_cache:


#docker-compose up --build
#sudo docker compose up --build
#docker exec -it ollama ollama pull mistral:instruct
#docker exec -it ollama ollama pull nomic-embed-text
#docker-compose down
#sudo docker compose down --remove-orphans
#docker-compose up

#sudo docker compose build --no-cache ingestion rag-agent weave-worker
#sudo docker compose run --rm ingestion

